venue,title,url
ACL 2005|METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments|https://aclanthology.org/W05-0909/
ACL 2012|LEPOR: A Robust Evaluation Metric for Machine Translation with Augmented Factors|https://aclanthology.org/C12-2044/
ACL 2022|Bleu: a Method for Automatic Evaluation of Machine Translation|https://aclanthology.org/P02-1040/
ACL 2022|Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words|https://aclanthology.org/2022.acl-short.45/
ACL 2022|Translation between Molecules and Natural Language|https://aclanthology.org/2022.emnlp-main.26/
ACM Computing Surveys 2023|Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing|https://dl.acm.org/doi/full/10.1145/3560815
ICLR 2015|Adam: A Method for Stochastic Optimization|https://arxiv.org/abs/1412.6980
ICLR 2019|Decoupled Weight Decay Regularization|https://arxiv.org/abs/1711.05101
ICLR 2021|Isotropy in the Contextual Embedding Space: Clusters and Manifolds|https://openreview.net/forum?id=xYGNO86OWDH
ICLR 2023|Self-Consistency Improves Chain of Thought Reasoning in Language Models|https://openreview.net/forum?id=1PL1NIMMrw
Nature 2023|Foundation models for generalist medical artificial intelligence|https://www.nature.com/articles/s41586-023-05881-4
Nature Communications 2022|Language models can learn complex molecular distributions|https://www.nature.com/articles/s41467-022-30839-x
NeurIPS 2017|Attention is All you Need|https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
NeurIPS 2022|Chain-of-Thought Prompting Elicits Reasoning in Large Language Models|https://openreview.net/forum?id=_VjQlMeSB_J
NeurIPS 2022|Training language models to follow instructions with human feedback|https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html
OpenAI 2018|Improving Language Understanding by Generative Pre-Training|https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
OpenAI 2019|Language Models are Unsupervised Multitask Learners|https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
OpenAI 2020|Language Models are Few-Shot Learners|https://arxiv.org/abs/2005.14165
OpenAI 2023|GPT-4 Technical Report,https://arxiv.org/abs/2303.08774
